{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Here you will find a series of docs that outline various procedures and how-tos when interacting with ArgoCD. Access the ArgoCD UI here .","title":"Home"},{"location":"argocd_metrics/","text":"Important Metrics We are tracking a number of metrics at our operations dashboard. What follows are the distinct categories of metrics Service Level Indicators SLIs are carefully defined quantitative measures of some aspect of the level of service that is provided. You can find more information on SLIs in the Site Reliability Engineering Book . SLI Query Status Availability min(up{job=\"ArgoCD Metrics\"}) Done Reconciliation Performance sum(increase(argocd_app_reconcile_bucket{dest_server=\\~\"$Server\", job=\"ArgoCD Metrics\"}[10m])) by (le) Needs Update to use Histogram Length of Work Queue increase(workqueue_unfinished_work_seconds{}[10m]) Done SLI Descriptions Availability: This tells us whether important ArgoCD pods are up. Reconciliation Performance: For ArgoCD, Reconciliation refers to making the state of the application match what is stored in git. This is usually done by managing Kubernetes objects. This SLI lets us see how much time our reconciliations are taking, and whether the system is performing in a degraded state/ needs to be scaled up. Length of Work Queue: This tells how many seconds of work are in progress but have not been observed by work_duration. Large values usually indicate stuck threads. Usage Metrics These metrics are being used to track usage of the ArgoCD instance Usage Metric Query Status Number of Managed Servers count(count by (dest_server) (argocd_app_info{project=\\~\"$Project\"})) Done Number of Managed Apps count((argocd_app_info{project=\\~\"$Project\"})) Done Number of Source Repos count(count by (repo) (argocd_app_info{project=\\~\"$Project\"})) Done Rate of Change To Be Implemented Not Done Service Level Indicators for App Owners These metrics are possible SLIs for the App Owners SLI Query Status App Sync Failures ceil(increase(argocd_app_sync_total{dest_server=\\~\"\\$Server\", project=\\~\"$Project\", phase=\\~\"Error|Failed\"}[10m])) Done Possible Service Level Indicators These metrics are interesting to look at but their usefulness has not yet been determined. Metric Query Possible Usage Status Percentage of Apps in Sync sum(argocd_app_info{sync_status=\"Synced\", dest_server=\\~\"\\$Server\", dest_namespace=\\~\"\\$Namespace\", project=\\~\"\\$Project\"})/sum(argocd_app_info{dest_server=\\~\"\\$Server\", dest_namespace=\\~\"\\$Namespace\", project=\\~\"\\$Project\"}) App Owner SLI Done Cumulative GRPC Success Rate sum(grpc_server_handled_total{grpc_code=\"OK\",job=\"ArgoCD Metrics\"})/sum(grpc_server_handled_total{job=\"ArgoCD Metrics\"}) SRE SLI Done Application Time Spent out of Sync To be Implemented App Owner SLI Not Done App Reconciliation Work Duration sum(increase(workqueue_work_duration_seconds_bucket{name=\"app_reconciliation_queue\"}[10m])) by (le) SRE SLI Update to use Histogram App Operation Processing Work Duration sum(increase(workqueue_work_duration_seconds_bucket{name=\"app_operation_processing_queue\"}[10m])) be (le) SRE SLI Update to use Histogram Metric Descriptions Percentage of Apps in Sync: Having a large percentage of apps be out of sync for a long duration indicates that there are some issues going on. Since the issues can be caused by both app-owners and the SREs it doesn't make sense to have this as an SRE SLI. Cumulative GRPC Success Rate: Seeing a large drop in the success rate could indicate issues with the underlying system. Application Time Spent out of Sync: Similar to 1 , this can have multiple causes so it is difficult to define an SRE SLI based on this. App Reconciliation Work Duration: This gives us a histogram of how much time (in seconds) it takes for ArgoCD to process items in the App Reconciliation queue. App Operation Processing Work Duration: This gives us a histogram of how much time (in seconds) it takes for ArgoCD to process items in the App Processing queue.","title":"Metrics"},{"location":"argocd_metrics/#important-metrics","text":"We are tracking a number of metrics at our operations dashboard. What follows are the distinct categories of metrics","title":"Important Metrics"},{"location":"argocd_metrics/#service-level-indicators","text":"SLIs are carefully defined quantitative measures of some aspect of the level of service that is provided. You can find more information on SLIs in the Site Reliability Engineering Book . SLI Query Status Availability min(up{job=\"ArgoCD Metrics\"}) Done Reconciliation Performance sum(increase(argocd_app_reconcile_bucket{dest_server=\\~\"$Server\", job=\"ArgoCD Metrics\"}[10m])) by (le) Needs Update to use Histogram Length of Work Queue increase(workqueue_unfinished_work_seconds{}[10m]) Done","title":"Service Level Indicators"},{"location":"argocd_metrics/#sli-descriptions","text":"Availability: This tells us whether important ArgoCD pods are up. Reconciliation Performance: For ArgoCD, Reconciliation refers to making the state of the application match what is stored in git. This is usually done by managing Kubernetes objects. This SLI lets us see how much time our reconciliations are taking, and whether the system is performing in a degraded state/ needs to be scaled up. Length of Work Queue: This tells how many seconds of work are in progress but have not been observed by work_duration. Large values usually indicate stuck threads.","title":"SLI Descriptions"},{"location":"argocd_metrics/#usage-metrics","text":"These metrics are being used to track usage of the ArgoCD instance Usage Metric Query Status Number of Managed Servers count(count by (dest_server) (argocd_app_info{project=\\~\"$Project\"})) Done Number of Managed Apps count((argocd_app_info{project=\\~\"$Project\"})) Done Number of Source Repos count(count by (repo) (argocd_app_info{project=\\~\"$Project\"})) Done Rate of Change To Be Implemented Not Done","title":"Usage Metrics"},{"location":"argocd_metrics/#service-level-indicators-for-app-owners","text":"These metrics are possible SLIs for the App Owners SLI Query Status App Sync Failures ceil(increase(argocd_app_sync_total{dest_server=\\~\"\\$Server\", project=\\~\"$Project\", phase=\\~\"Error|Failed\"}[10m])) Done","title":"Service Level Indicators for App Owners"},{"location":"argocd_metrics/#possible-service-level-indicators","text":"These metrics are interesting to look at but their usefulness has not yet been determined. Metric Query Possible Usage Status Percentage of Apps in Sync sum(argocd_app_info{sync_status=\"Synced\", dest_server=\\~\"\\$Server\", dest_namespace=\\~\"\\$Namespace\", project=\\~\"\\$Project\"})/sum(argocd_app_info{dest_server=\\~\"\\$Server\", dest_namespace=\\~\"\\$Namespace\", project=\\~\"\\$Project\"}) App Owner SLI Done Cumulative GRPC Success Rate sum(grpc_server_handled_total{grpc_code=\"OK\",job=\"ArgoCD Metrics\"})/sum(grpc_server_handled_total{job=\"ArgoCD Metrics\"}) SRE SLI Done Application Time Spent out of Sync To be Implemented App Owner SLI Not Done App Reconciliation Work Duration sum(increase(workqueue_work_duration_seconds_bucket{name=\"app_reconciliation_queue\"}[10m])) by (le) SRE SLI Update to use Histogram App Operation Processing Work Duration sum(increase(workqueue_work_duration_seconds_bucket{name=\"app_operation_processing_queue\"}[10m])) be (le) SRE SLI Update to use Histogram","title":"Possible Service Level Indicators"},{"location":"argocd_metrics/#metric-descriptions","text":"Percentage of Apps in Sync: Having a large percentage of apps be out of sync for a long duration indicates that there are some issues going on. Since the issues can be caused by both app-owners and the SREs it doesn't make sense to have this as an SRE SLI. Cumulative GRPC Success Rate: Seeing a large drop in the success rate could indicate issues with the underlying system. Application Time Spent out of Sync: Similar to 1 , this can have multiple causes so it is difficult to define an SRE SLI based on this. App Reconciliation Work Duration: This gives us a histogram of how much time (in seconds) it takes for ArgoCD to process items in the App Reconciliation queue. App Operation Processing Work Duration: This gives us a histogram of how much time (in seconds) it takes for ArgoCD to process items in the App Processing queue.","title":"Metric Descriptions"},{"location":"argocd_project_permissions/","text":"ArgoCD Permissions Currently there are 3 OpenShift groups (same as ldap) with access: data-hub-openshift-admins aicoe-thoth-devops aicoe-aiops-devops There are 3 ArgoCD roles with defined policies: thoth-admin data-hub-admin aiops-admin OpenShift groups have the following ArgoCD role associations: OC Group ArgoCD Role aicoe-thoth-devops thoth-admin data-hub-openshift-admins data-hub-admin aicoe-aiops-devops aiops-admin ArgoCD roles have policies that define their access to ArgoCD resources, they are defined in policy.csv . Read about how these policies and roles work here . In summary this is what you can and cannot do as a <project>-admin : get/create/update argocd clusters, certs, repos, projects, accounts cannot delete argocd clusters, certs, repos, projects, accounts this means <project>-admin can do things like argocd cluster add ... or argocd proj list ... <project>-admin can do pretty much anything including delete argocd application in their ArgoCD <project> <project>-admin cannot delete non application resources All this applies to OpenShift Authentication, ArgoCD admin account retains unrestricted access.","title":"Permissions"},{"location":"argocd_project_permissions/#argocd-permissions","text":"Currently there are 3 OpenShift groups (same as ldap) with access: data-hub-openshift-admins aicoe-thoth-devops aicoe-aiops-devops There are 3 ArgoCD roles with defined policies: thoth-admin data-hub-admin aiops-admin OpenShift groups have the following ArgoCD role associations: OC Group ArgoCD Role aicoe-thoth-devops thoth-admin data-hub-openshift-admins data-hub-admin aicoe-aiops-devops aiops-admin ArgoCD roles have policies that define their access to ArgoCD resources, they are defined in policy.csv . Read about how these policies and roles work here . In summary this is what you can and cannot do as a <project>-admin : get/create/update argocd clusters, certs, repos, projects, accounts cannot delete argocd clusters, certs, repos, projects, accounts this means <project>-admin can do things like argocd cluster add ... or argocd proj list ... <project>-admin can do pretty much anything including delete argocd application in their ArgoCD <project> <project>-admin cannot delete non application resources All this applies to OpenShift Authentication, ArgoCD admin account retains unrestricted access.","title":"ArgoCD Permissions"},{"location":"create_argocd_application_manifest/","text":"Application Management While ArgoCD allows you to create ArgoCD applications via the UI and CLI, it is our policy that all applications be created declaratively . This allows us to easily restore your applications should the need arise. Pre-requisites Kustomize version 3.8+ Steps for creating an application For your application to show up to ArgoCD you need to do 2 things: 1. Create the Application yaml in the appropriate path in a fork 2. Submit a PR to this repository These steps are outlined in detail below: Step 1. Create the Application Yaml Clone the repo and cd into where applications are stored: $ git clone https://github.com/${GIT_USERNAME}/aicoe-cd.git $ cd aicoe-cd/manifests/overlays/prod/privileged_resources/applications If your team folder does not exist, create it: $ mkdir example_team && cd example_team $ kustomize create Let's create a sample application called example-app . # aicoe-cd/manifests/overlays/prod/privileged_resources/applications/example_team/example_app.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: example_app spec: destination: namespace: example_namespace server: http://example_server project: example_project source: path: path/to/kustomization repoURL: http://path-to-example-repo targetRevision: HEAD syncPolicy: automated: prune: true selfHeal: true syncOptions: - Validate=false This is a basic minimal example application. For additional fields see here . Let's go over what some of the fields in the example_app.yaml refer to: metadata.name : the name of the Application resource as well as the name as it appears on the ui. spec.project : the ArgoCD Project to which this Application belongs, ensure that this Project exists in projects . spec.destination.namespace : the target namespace for this Application 's deployment, ensure that this namespace exists in clusters for the appropriate cluster/server. spec.destination.cluster : the target cluster server name for this Application 's deployment, ensure this server exists in clusters for the appropriate cluster. spec.source.path : path to the Kustomization.yaml file relative to the repo's root. spec.source.repoURL : the repository holding the Application 's desired state, ensure this repo exists within argo_cm here . In case you are adding a private repo: Please ensure @sesheta bot has access to your repo (directly or indirectly via @AICoE/sourceops / @aicoe-aiops/sourceops teams) Reference repository access credentials via SourceOps secret: yaml repositories: | ... - type: git url: https://github.com/<PATH>.git usernameSecret: key: username secret: srcops passwordSecret: key: token secret: srcops ... NOTE: You may need to disable schema validation if your deployments are failing. This is due to the ArgoCD api validator having a very strict api spec. Once you are done add the application yaml to the Kustomization file by running the following from the repository root directory . $ cd aicoe-cd/manifests/overlays/prod/privileged_resources/applications/example_team $ kustomize edit add resource example_app.yaml Step 2. Make a Pull request Commit your changes and submit a pr to aicoe-sre repository. An AICOE-SRE team member will review your PR. If your application exists in ArgoCD but not on VCS We make no guarantees about your application if it does not exist on vcs. Our policy is \" if it's not on vcs, it does not exist \". Luckily, getting the manifests for all your applications is easy to do with the argocd cli. # Login via cli using sso $ argocd --insecure --grpc-web login ${ARGOCD_ROUTE}:443 --sso # Get the application resource details $ argocd app get ${APP_NAME} -o yaml","title":"Create ArgoCD application manifest"},{"location":"create_argocd_application_manifest/#application-management","text":"While ArgoCD allows you to create ArgoCD applications via the UI and CLI, it is our policy that all applications be created declaratively . This allows us to easily restore your applications should the need arise.","title":"Application Management"},{"location":"create_argocd_application_manifest/#pre-requisites","text":"Kustomize version 3.8+","title":"Pre-requisites"},{"location":"create_argocd_application_manifest/#steps-for-creating-an-application","text":"For your application to show up to ArgoCD you need to do 2 things: 1. Create the Application yaml in the appropriate path in a fork 2. Submit a PR to this repository These steps are outlined in detail below:","title":"Steps for creating an application"},{"location":"create_argocd_application_manifest/#step-1-create-the-application-yaml","text":"Clone the repo and cd into where applications are stored: $ git clone https://github.com/${GIT_USERNAME}/aicoe-cd.git $ cd aicoe-cd/manifests/overlays/prod/privileged_resources/applications If your team folder does not exist, create it: $ mkdir example_team && cd example_team $ kustomize create Let's create a sample application called example-app . # aicoe-cd/manifests/overlays/prod/privileged_resources/applications/example_team/example_app.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: example_app spec: destination: namespace: example_namespace server: http://example_server project: example_project source: path: path/to/kustomization repoURL: http://path-to-example-repo targetRevision: HEAD syncPolicy: automated: prune: true selfHeal: true syncOptions: - Validate=false This is a basic minimal example application. For additional fields see here . Let's go over what some of the fields in the example_app.yaml refer to: metadata.name : the name of the Application resource as well as the name as it appears on the ui. spec.project : the ArgoCD Project to which this Application belongs, ensure that this Project exists in projects . spec.destination.namespace : the target namespace for this Application 's deployment, ensure that this namespace exists in clusters for the appropriate cluster/server. spec.destination.cluster : the target cluster server name for this Application 's deployment, ensure this server exists in clusters for the appropriate cluster. spec.source.path : path to the Kustomization.yaml file relative to the repo's root. spec.source.repoURL : the repository holding the Application 's desired state, ensure this repo exists within argo_cm here . In case you are adding a private repo: Please ensure @sesheta bot has access to your repo (directly or indirectly via @AICoE/sourceops / @aicoe-aiops/sourceops teams) Reference repository access credentials via SourceOps secret: yaml repositories: | ... - type: git url: https://github.com/<PATH>.git usernameSecret: key: username secret: srcops passwordSecret: key: token secret: srcops ... NOTE: You may need to disable schema validation if your deployments are failing. This is due to the ArgoCD api validator having a very strict api spec. Once you are done add the application yaml to the Kustomization file by running the following from the repository root directory . $ cd aicoe-cd/manifests/overlays/prod/privileged_resources/applications/example_team $ kustomize edit add resource example_app.yaml","title":"Step 1. Create the Application Yaml"},{"location":"create_argocd_application_manifest/#step-2-make-a-pull-request","text":"Commit your changes and submit a pr to aicoe-sre repository. An AICOE-SRE team member will review your PR.","title":"Step 2. Make a Pull request"},{"location":"create_argocd_application_manifest/#if-your-application-exists-in-argocd-but-not-on-vcs","text":"We make no guarantees about your application if it does not exist on vcs. Our policy is \" if it's not on vcs, it does not exist \". Luckily, getting the manifests for all your applications is easy to do with the argocd cli. # Login via cli using sso $ argocd --insecure --grpc-web login ${ARGOCD_ROUTE}:443 --sso # Get the application resource details $ argocd app get ${APP_NAME} -o yaml","title":"If your application exists in ArgoCD but not on VCS"},{"location":"get_argocd_to_manage_your_app/","text":"Get ArgoCD to manage your Application When migrating an application's deployment to be managed by ArgoCD use the following checklist to verify your process. Ensure your application manifests can be built using Kustomize. Specify your apiVersion in your manifests with the apigroup included (e.g. for Deployments use apps.openshift.io/v1 instead of just v1 ) If using secrets, make sure to include the .sops.yaml file in your repository. See here for more info. Create the role granting access to namespace. See here for more info. This role should be tracked in your application manifest repository. If adding this role as part of the PR below, make sure not to include it in the kustomization.yaml file so that ArgoCD does not deploy it (due to lack of permissions) The following items require a PR to the aicoe-cd repository, and require an aicoe-sre team member to merge/deploy changes: Ensure the application repository is added in the repository file in repositories . Ensure that all OCP resources that will be managed by ArgoCD on this cluster are included in the inclusions list in resource.inclusions . See here for more info. Create the ArgoCD Application manifest See here for more info. The following require an aicoe-sre team member to make the changes: Ensure your namespace exists in your cluster's spec see here for details. If you are switching between ArgoCD managed namespaces, and that namespace was deleted in OCP, then ensure it's also removed from your cluster's credentials found here clusters .","title":"Get ArgoCD to manage your app"},{"location":"get_argocd_to_manage_your_app/#get-argocd-to-manage-your-application","text":"When migrating an application's deployment to be managed by ArgoCD use the following checklist to verify your process. Ensure your application manifests can be built using Kustomize. Specify your apiVersion in your manifests with the apigroup included (e.g. for Deployments use apps.openshift.io/v1 instead of just v1 ) If using secrets, make sure to include the .sops.yaml file in your repository. See here for more info. Create the role granting access to namespace. See here for more info. This role should be tracked in your application manifest repository. If adding this role as part of the PR below, make sure not to include it in the kustomization.yaml file so that ArgoCD does not deploy it (due to lack of permissions) The following items require a PR to the aicoe-cd repository, and require an aicoe-sre team member to merge/deploy changes: Ensure the application repository is added in the repository file in repositories . Ensure that all OCP resources that will be managed by ArgoCD on this cluster are included in the inclusions list in resource.inclusions . See here for more info. Create the ArgoCD Application manifest See here for more info. The following require an aicoe-sre team member to make the changes: Ensure your namespace exists in your cluster's spec see here for details. If you are switching between ArgoCD managed namespaces, and that namespace was deleted in OCP, then ensure it's also removed from your cluster's credentials found here clusters .","title":"Get ArgoCD to manage your Application"},{"location":"give_argocd_access_to_your_project/","text":"Give ArgoCD access to your project ArgoCD uses an SA named argocd-manager to deploy resources to another cluster/namespace. These SAs need access to the resources it will be deploying, this is done via roles and rolebindings. In your namespace, you will need to deploy a rolebinding like the one below: apiVersion: authorization.openshift.io/v1 kind: RoleBinding metadata: name: argocd-manager-rolebinding namespace: <application_namespace> roleRef: name: <role> apiGroup: rbac.authorization.k8s.io kind: ClusterRole subjects: - kind: ServiceAccount name: argocd-manager namespace: <sa_namespace> Fill out application_namespace , role , and sa_namespace . application_namespace : This is your project namespace. sa_namespace : On PSI OCP4 this will be aicoe-argocd and on Data-Hub clusters it will be argocd-manager . role : must be a project admin role.","title":"Give ArgoCD access to your project"},{"location":"give_argocd_access_to_your_project/#give-argocd-access-to-your-project","text":"ArgoCD uses an SA named argocd-manager to deploy resources to another cluster/namespace. These SAs need access to the resources it will be deploying, this is done via roles and rolebindings. In your namespace, you will need to deploy a rolebinding like the one below: apiVersion: authorization.openshift.io/v1 kind: RoleBinding metadata: name: argocd-manager-rolebinding namespace: <application_namespace> roleRef: name: <role> apiGroup: rbac.authorization.k8s.io kind: ClusterRole subjects: - kind: ServiceAccount name: argocd-manager namespace: <sa_namespace> Fill out application_namespace , role , and sa_namespace . application_namespace : This is your project namespace. sa_namespace : On PSI OCP4 this will be aicoe-argocd and on Data-Hub clusters it will be argocd-manager . role : must be a project admin role.","title":"Give ArgoCD access to your project"},{"location":"inclusions_explained/","text":"Inclusions explained It is likely that your team does not have get access to all namespace scoped resources. This can be an issue when deploying apps to a namespace in a cluster, because ArgoCD will attempt to discover all namespace scoped resourced and be denied. To avoid this, we limit ArgoCD to discover the resources that are available to project admins, these are added under the resource.inclusions ArgoCD configurations in resource.inclusions If your application contains resources that a project admin does not have permissions to list/edit then you can request that a cluster admin deploy aggregated roles to add such permissions. See here for an example . Once having done so, you can make a PR with these resources added onto the resource.inclusions list in resource.inclusions .","title":"Inclusions Explained"},{"location":"inclusions_explained/#inclusions-explained","text":"It is likely that your team does not have get access to all namespace scoped resources. This can be an issue when deploying apps to a namespace in a cluster, because ArgoCD will attempt to discover all namespace scoped resourced and be denied. To avoid this, we limit ArgoCD to discover the resources that are available to project admins, these are added under the resource.inclusions ArgoCD configurations in resource.inclusions If your application contains resources that a project admin does not have permissions to list/edit then you can request that a cluster admin deploy aggregated roles to add such permissions. See here for an example . Once having done so, you can make a PR with these resources added onto the resource.inclusions list in resource.inclusions .","title":"Inclusions explained"},{"location":"manage_your_app_secrets/","text":"Secret Management Secret management is handled using the KSOPs plugin. Use sops to encrypt your secrets in vcs. Use the AICOE-SRE public sops key to encrypt your secrets so that ArgoCD may use KSOPs to decrypt them. Overview: KSOPs KSOPS , or kustomize-SOPS, is a kustomize plugin for SOPS encrypted resources. KSOPS can be used to decrypt any Kubernetes resource, but is most commonly used to decrypt encrypted Kubernetes Secrets and ConfigMaps. As a kustomize plugin, KSOPS allows you to manage, build, and apply encrypted manifests the same way you manage the rest of your Kubernetes manifests. Requirements Go kustomize built with Go (See details below ) SOPS gpg See versions to download the appropriate version of SOPS, Kustomize, and KSOPS. 0. Verify Requirements Before continuing, verify your installation of Go , SOPS , and gpg . Below are a few non-comprehensive commands to quickly check your installations: # Verify that the latest version of Go is installed i.e. v1.13 and above go version # Verify that your $GOPATH is set go env # Verify SOPS is installed sops --version # Verify gpg is installed gpg --help 1. Download KSOPS # export GO111MODULE=on go get -u github.com/viaduct-ai/kustomize-sops # cd into the root directory cd $GOPATH/src/github.com/viaduct-ai/kustomize-sops 2. Install (or Reinstall) the Latest kustomize via Go # KSOPS is built with latest kustomize # If you want to change versions, update the installation script with your desired version and make sure to check that the KSOPS tests still pass # If you want to change versions below kustomize v3.3.0, use the KSOPS v1.0 or go-1.12 release! make kustomize 3. Setup kustomize Plugin Path # Don't forget to define XDG_CONFIG_HOME in your .bashrc/.zshrc echo \"export XDG_CONFIG_HOME=\\$HOME/.config\" >> $HOME/.bashrc source $HOME/.bashrc 4. Build and Install KSOPS Plugin make install 5. Configure SOPS via .sops.yaml KSOPS relies on the SOPS creation rules defined in .sops.yaml . To make encrypted secrets more readable, we suggest using the following encryption regex to only encrypt data and stringData values. This leaves non-sensitive fields, like the secret's name, unencrypted and human readable. You will have to modify .sops.yaml if you want to use your key management service by providing the correct gpg fingerprint. You can customize this file according to the type of secrets you want to encrypt. creation_rules: - encrypted_regex: '^(data|stringData)$' # Specify kms/pgp/etc encryption key pgp: '<gpg-fingerprint>' # Optionally you can configure to use a providers key store # kms: XXXXXX # gcp_kms: XXXXXX 6. Create a Resource # Create a local Kubernetes Secret cat <<EOF > secret.yaml apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm EOF 7. Encrypt the Resource # Encrypt with SOPS CLI # Specify SOPS configuration in .sops.yaml sops -e secret.yaml > secret.enc.yaml 8. Define KSOPS kustomize Generator # Create a local Kubernetes Secret cat <<EOF > secret-generator.yaml apiVersion: viaduct.ai/v1 kind: ksops metadata: # Specify a name name: example-secret-generator files: - ./secret.enc.yaml EOF 9. Create the kustomization.yaml Read about kustomize plugins cat <<EOF > kustomization.yaml generators: - ./secret-generator.yaml EOF","title":"Manage Your App Secrets"},{"location":"manage_your_app_secrets/#secret-management","text":"Secret management is handled using the KSOPs plugin. Use sops to encrypt your secrets in vcs. Use the AICOE-SRE public sops key to encrypt your secrets so that ArgoCD may use KSOPs to decrypt them.","title":"Secret Management"},{"location":"manage_your_app_secrets/#overview-ksops","text":"KSOPS , or kustomize-SOPS, is a kustomize plugin for SOPS encrypted resources. KSOPS can be used to decrypt any Kubernetes resource, but is most commonly used to decrypt encrypted Kubernetes Secrets and ConfigMaps. As a kustomize plugin, KSOPS allows you to manage, build, and apply encrypted manifests the same way you manage the rest of your Kubernetes manifests.","title":"Overview: KSOPs"},{"location":"manage_your_app_secrets/#requirements","text":"Go kustomize built with Go (See details below ) SOPS gpg See versions to download the appropriate version of SOPS, Kustomize, and KSOPS.","title":"Requirements"},{"location":"manage_your_app_secrets/#0-verify-requirements","text":"Before continuing, verify your installation of Go , SOPS , and gpg . Below are a few non-comprehensive commands to quickly check your installations: # Verify that the latest version of Go is installed i.e. v1.13 and above go version # Verify that your $GOPATH is set go env # Verify SOPS is installed sops --version # Verify gpg is installed gpg --help","title":"0. Verify Requirements"},{"location":"manage_your_app_secrets/#1-download-ksops","text":"# export GO111MODULE=on go get -u github.com/viaduct-ai/kustomize-sops # cd into the root directory cd $GOPATH/src/github.com/viaduct-ai/kustomize-sops","title":"1. Download KSOPS"},{"location":"manage_your_app_secrets/#2-install-or-reinstall-the-latest-kustomize-via-go","text":"# KSOPS is built with latest kustomize # If you want to change versions, update the installation script with your desired version and make sure to check that the KSOPS tests still pass # If you want to change versions below kustomize v3.3.0, use the KSOPS v1.0 or go-1.12 release! make kustomize","title":"2. Install (or Reinstall) the Latest kustomize via Go"},{"location":"manage_your_app_secrets/#3-setup-kustomize-plugin-path","text":"# Don't forget to define XDG_CONFIG_HOME in your .bashrc/.zshrc echo \"export XDG_CONFIG_HOME=\\$HOME/.config\" >> $HOME/.bashrc source $HOME/.bashrc","title":"3. Setup kustomize Plugin Path"},{"location":"manage_your_app_secrets/#4-build-and-install-ksops-plugin","text":"make install","title":"4. Build and Install KSOPS Plugin"},{"location":"manage_your_app_secrets/#5-configure-sops-via-sopsyaml","text":"KSOPS relies on the SOPS creation rules defined in .sops.yaml . To make encrypted secrets more readable, we suggest using the following encryption regex to only encrypt data and stringData values. This leaves non-sensitive fields, like the secret's name, unencrypted and human readable. You will have to modify .sops.yaml if you want to use your key management service by providing the correct gpg fingerprint. You can customize this file according to the type of secrets you want to encrypt. creation_rules: - encrypted_regex: '^(data|stringData)$' # Specify kms/pgp/etc encryption key pgp: '<gpg-fingerprint>' # Optionally you can configure to use a providers key store # kms: XXXXXX # gcp_kms: XXXXXX","title":"5. Configure SOPS via .sops.yaml"},{"location":"manage_your_app_secrets/#6-create-a-resource","text":"# Create a local Kubernetes Secret cat <<EOF > secret.yaml apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm EOF","title":"6. Create a Resource"},{"location":"manage_your_app_secrets/#7-encrypt-the-resource","text":"# Encrypt with SOPS CLI # Specify SOPS configuration in .sops.yaml sops -e secret.yaml > secret.enc.yaml","title":"7. Encrypt the Resource"},{"location":"manage_your_app_secrets/#8-define-ksops-kustomize-generator","text":"# Create a local Kubernetes Secret cat <<EOF > secret-generator.yaml apiVersion: viaduct.ai/v1 kind: ksops metadata: # Specify a name name: example-secret-generator files: - ./secret.enc.yaml EOF","title":"8. Define KSOPS kustomize Generator"},{"location":"manage_your_app_secrets/#9-create-the-kustomizationyaml","text":"Read about kustomize plugins cat <<EOF > kustomization.yaml generators: - ./secret-generator.yaml EOF","title":"9. Create the kustomization.yaml"},{"location":"setup_argocd_dev_environment/","text":"Deploying a development environment Prequisites An OCP 4.x Development cluster. Must have cluster admin. Follow the steps here . Instructions Create the project aicoe-argocd-dev and argocd-test . The latter will be used for deploying a dev application via ArgoCD. oc new-project argocd-test oc new-project aicoe-argocd-dev Deploy ArgoCD git clone git@github.com:AICoE/aicoe-cd.git cd aicoe-cd # Deploy Cluser objects kustomize build manifests/crds --enable_alpha_plugins | oc apply -f - # Deploy Non Cluster objects kustomize build manifests/overlays/dev --enable_alpha_plugins | oc apply -f - Configure Auth Once deployed, there are some additional configurations, run this script: examples/configure_development.sh Feel free to look inside the script for detailed comments on what configurations are applied. Cleanup Run the following commands to clean up your environment. kustomize build manifests/overlays/dev --enable_alpha_plugins | oc delete -f - kustomize build manifests/crds --enable_alpha_plugins | oc delete -f - oc delete group dev oc delete project argocd-test oc delete project aicoe-argocd-dev You may ignore the following error when removing secrets: Error from server (NotFound): error when deleting \"STDIN\": secrets \"argocd-dex-server-oauth-token\" not found Error from server (NotFound): error when deleting \"STDIN\": secrets \"dev-cluster-spec\" not found","title":"Configuring Environment"},{"location":"setup_argocd_dev_environment/#deploying-a-development-environment","text":"","title":"Deploying a development environment"},{"location":"setup_argocd_dev_environment/#prequisites","text":"An OCP 4.x Development cluster. Must have cluster admin. Follow the steps here .","title":"Prequisites"},{"location":"setup_argocd_dev_environment/#instructions","text":"Create the project aicoe-argocd-dev and argocd-test . The latter will be used for deploying a dev application via ArgoCD. oc new-project argocd-test oc new-project aicoe-argocd-dev Deploy ArgoCD git clone git@github.com:AICoE/aicoe-cd.git cd aicoe-cd # Deploy Cluser objects kustomize build manifests/crds --enable_alpha_plugins | oc apply -f - # Deploy Non Cluster objects kustomize build manifests/overlays/dev --enable_alpha_plugins | oc apply -f -","title":"Instructions"},{"location":"setup_argocd_dev_environment/#configure-auth","text":"Once deployed, there are some additional configurations, run this script: examples/configure_development.sh Feel free to look inside the script for detailed comments on what configurations are applied.","title":"Configure Auth"},{"location":"setup_argocd_dev_environment/#cleanup","text":"Run the following commands to clean up your environment. kustomize build manifests/overlays/dev --enable_alpha_plugins | oc delete -f - kustomize build manifests/crds --enable_alpha_plugins | oc delete -f - oc delete group dev oc delete project argocd-test oc delete project aicoe-argocd-dev You may ignore the following error when removing secrets: Error from server (NotFound): error when deleting \"STDIN\": secrets \"argocd-dex-server-oauth-token\" not found Error from server (NotFound): error when deleting \"STDIN\": secrets \"dev-cluster-spec\" not found","title":"Cleanup"},{"location":"versions/","text":"Versions ArgoCD: 1.7.4 KSOPs: 2.1.4 Kustomize: 3.8.0 SOPS: 3.6.0+ The KSOPS and Kustomize versions refer to the ones provisioned with ArgoCD. Kustomize versions can be adjusted manually using customized versions .","title":"Versions"},{"location":"versions/#versions","text":"ArgoCD: 1.7.4 KSOPs: 2.1.4 Kustomize: 3.8.0 SOPS: 3.6.0+ The KSOPS and Kustomize versions refer to the ones provisioned with ArgoCD. Kustomize versions can be adjusted manually using customized versions .","title":"Versions"},{"location":"admin/add_namespace_to_cluster/","text":"Add namespace to cluster Prerequisites sops 3.6+ imported aicoe-sre gpg key Instructions Namespaces are added to ArgoCD by altering the corresponding cluster spec. Cluster specs are defined within the clusters folder. Open the file in the sops editor, for example if updating the cluster spec apps.ocp.prod.psi.redhat.com.enc.yaml you would execute: # From repo root $ cd manifests/overlays/prod/resources/secrets/clusters $ sops apps.ocp.prod.psi.redhat.com.enc.yaml This should open the decrypted form of the cluster spec. Update the namespace field by appending your namespace (comma-separated, no spaces, if there are multiple namespaces). ... 10 stringData: 11 name: apps.ocp.prod.psi.redhat.com 12 config: ... 14 namespaces: namespace-1,namespace-2 # Update this field by appending the new namespace 15 server: ... Once done, submit a PR with this change. If your cluster isn't there then it will need to be added by a member of the aicoe-sre team. See here for more info.","title":"Add namespace to cluster"},{"location":"admin/add_namespace_to_cluster/#add-namespace-to-cluster","text":"","title":"Add namespace to cluster"},{"location":"admin/add_namespace_to_cluster/#prerequisites","text":"sops 3.6+ imported aicoe-sre gpg key","title":"Prerequisites"},{"location":"admin/add_namespace_to_cluster/#instructions","text":"Namespaces are added to ArgoCD by altering the corresponding cluster spec. Cluster specs are defined within the clusters folder. Open the file in the sops editor, for example if updating the cluster spec apps.ocp.prod.psi.redhat.com.enc.yaml you would execute: # From repo root $ cd manifests/overlays/prod/resources/secrets/clusters $ sops apps.ocp.prod.psi.redhat.com.enc.yaml This should open the decrypted form of the cluster spec. Update the namespace field by appending your namespace (comma-separated, no spaces, if there are multiple namespaces). ... 10 stringData: 11 name: apps.ocp.prod.psi.redhat.com 12 config: ... 14 namespaces: namespace-1,namespace-2 # Update this field by appending the new namespace 15 server: ... Once done, submit a PR with this change. If your cluster isn't there then it will need to be added by a member of the aicoe-sre team. See here for more info.","title":"Instructions"},{"location":"admin/add_new_cluster_spec/","text":"Adding a new cluster spec Prerequisites sops 3.6+ imported aicoe-sre gpg key Instructions ArgoCD will need a service account present on the cluster for deployments. Where the SA is located is irrelevant, though it's advised to have it be located in its own independent namespace. For consistency name this service account argocd-manager . This workflow may look like this: oc login <your_cluster> oc new-project argocd-manager oc create sa argocd-manager Get the token for this SA SA_TOKEN=`oc sa get-token argocd-manager -n argocd-manager` Create the cluster spec: # manifests/overlays/prod/resources/secrets/clusters/datahub.psi.redhat.com.yaml apiVersion: v1 kind: Secret metadata: name: cluster-datahub.psi.redhat.com labels: argocd.argoproj.io/secret-type: cluster annotations: managed-by: argocd.argoproj.io type: Opaque stringData: name: datahub.psi.redhat.com config: | {\"bearerToken\": ${SA_TOKEN}, \"tlsClientConfig\": {\"insecure\": true}} namespaces: namespace_1,namespace_2 server: https://datahub.psi.redhat.com:44 Let's go over what some of the fields in the stringData field refer to: name : Name for this cluster, appears in the ArgoCD UI config : The token goes here, replace the contents of ${SA_TOKEN} with the one retrieved earlier. namespace : List of namespaces ArgoCD has permissions to deploy to, comma-separated, no whitespace. NOTE : Every namespace listed in this field must have permissions for ArgoCD to manage the deployments. Refer to this document to set up permissions to the namespace. server : Cluster api server hostname, can be retrieved by running oc whoami --show-server SOPS encrypt the manifest and store it in the clusters directory. $ cd manifests/overlays/prod/resources/secrets/clusters/ $ sops -e datahub.psi.redhat.com.yaml > datahub.psi.redhat.com.enc.yaml # Delete the cluster_spec.yaml (decrypted version) $ rm datahub.psi.redhat.com.yaml Once done, submit a PR. Note: DO NOT submit a PR with the decrypted cluster spec secret.","title":"Adding a new cluster spec"},{"location":"admin/add_new_cluster_spec/#adding-a-new-cluster-spec","text":"","title":"Adding a new cluster spec"},{"location":"admin/add_new_cluster_spec/#prerequisites","text":"sops 3.6+ imported aicoe-sre gpg key","title":"Prerequisites"},{"location":"admin/add_new_cluster_spec/#instructions","text":"ArgoCD will need a service account present on the cluster for deployments. Where the SA is located is irrelevant, though it's advised to have it be located in its own independent namespace. For consistency name this service account argocd-manager . This workflow may look like this: oc login <your_cluster> oc new-project argocd-manager oc create sa argocd-manager Get the token for this SA SA_TOKEN=`oc sa get-token argocd-manager -n argocd-manager` Create the cluster spec: # manifests/overlays/prod/resources/secrets/clusters/datahub.psi.redhat.com.yaml apiVersion: v1 kind: Secret metadata: name: cluster-datahub.psi.redhat.com labels: argocd.argoproj.io/secret-type: cluster annotations: managed-by: argocd.argoproj.io type: Opaque stringData: name: datahub.psi.redhat.com config: | {\"bearerToken\": ${SA_TOKEN}, \"tlsClientConfig\": {\"insecure\": true}} namespaces: namespace_1,namespace_2 server: https://datahub.psi.redhat.com:44 Let's go over what some of the fields in the stringData field refer to: name : Name for this cluster, appears in the ArgoCD UI config : The token goes here, replace the contents of ${SA_TOKEN} with the one retrieved earlier. namespace : List of namespaces ArgoCD has permissions to deploy to, comma-separated, no whitespace. NOTE : Every namespace listed in this field must have permissions for ArgoCD to manage the deployments. Refer to this document to set up permissions to the namespace. server : Cluster api server hostname, can be retrieved by running oc whoami --show-server SOPS encrypt the manifest and store it in the clusters directory. $ cd manifests/overlays/prod/resources/secrets/clusters/ $ sops -e datahub.psi.redhat.com.yaml > datahub.psi.redhat.com.enc.yaml # Delete the cluster_spec.yaml (decrypted version) $ rm datahub.psi.redhat.com.yaml Once done, submit a PR. Note: DO NOT submit a PR with the decrypted cluster spec secret.","title":"Instructions"},{"location":"admin/update_gpg_key/","text":"Update gpg key Prerequisites sops 3.6+ imported aicoe-sre gpg key Instructions Export the key $ gpg --export-secret-keys \"${KEY_ID}\" | base64 > private.asc # From the repo root $ cd manifests/overlays/prod/resources/secrets/gpg $ sops secret.enc.yaml Copy the contents of private.asc into the private.key field. Save the file, exit. Commit and make a PR.","title":"Update gpg key"},{"location":"admin/update_gpg_key/#update-gpg-key","text":"","title":"Update gpg key"},{"location":"admin/update_gpg_key/#prerequisites","text":"sops 3.6+ imported aicoe-sre gpg key","title":"Prerequisites"},{"location":"admin/update_gpg_key/#instructions","text":"Export the key $ gpg --export-secret-keys \"${KEY_ID}\" | base64 > private.asc # From the repo root $ cd manifests/overlays/prod/resources/secrets/gpg $ sops secret.enc.yaml Copy the contents of private.asc into the private.key field. Save the file, exit. Commit and make a PR.","title":"Instructions"}]}